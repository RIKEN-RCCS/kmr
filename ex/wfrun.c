/* wfrun.c (2016-07-14) -*-Coding: us-ascii;-*- */

/** \file wfrun.c Simple Workflow.  It runs jobs of MPI according to a
    job description generated by Python code.  The master code is in
    Python for usability, while the worker code is in C.  To make it a
    single program, it invokes a Python interpreter for the master
    inside.  The worker code needs to be in C because it needs to
    enlarge the sizes of DATA/BSS and TLS.  Note the master rank is
    (nprocs-1).  The part of the code invoking an interpreter is taken
    from the examples in
    "https://docs.python.org/2/extending/embedding.html".  Compile
    this with "cc -I/usr/include/python2.6 wfrun.c -lpython2.6".

    RUNNING: (1) First, it needs Python and its library "mpi4py".
    Source the "hpcu" environment on K.  Second, it needs KMR
    libraries "libkmr.so", "libkmrspawn.so", and "kmr4py.py".  Copy
    them and stage-in them, for example, in the current directory, and
    set the environment variables.

    |$ . /work/system/Env_base|
    |$ . /opt/aics/hpcu/env.sh|
    |$ export LD_LIBRARY_PATH=.:$LD_LIBRARY_PATH|
    |$ export PYTHONPATH=.:PYTHONPATH|
    |$ mpiexec ./wfrun python-script-for-master| */

/* Reserve DATA and TLS spaces.  Use compiler options to change the
   sizes: -DDAT="512 MB" -DTLS="64 KB". */

#define KB * 1024
#define MB * 1024 * 1024
#ifndef DAT
#define DAT (32 MB)
#endif
#ifndef TLS
#define TLS (8 KB)
#endif

char _kmr_dat_[DAT];
__thread /*_Thread_local*/ char _kmr_tls_[TLS];

#undef DAT
#undef TLS
#undef MB
#undef KB

#include <Python.h>
#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <errno.h>
#include "kmr.h"

/* NOTE#0: It only start MPI4PY on the master and avoids it on
   workers.  It checks the environment variable OMPI_MCA_orte_ess_vpid
   which holds a rank of a process on Open-MPI. */

int
main(int argc, char *argv[])
{
    /* Check the rank, not using MPI.  It wants to use MPI4PY on the
       master only.  Workers are simply to serve spawning.  Use
       Open-MPI specific environment variables. */

    int vpid;
    int nprocs;
    {
	char gomi[4];

	char *e0 = "OMPI_MCA_orte_ess_vpid";
	char *s0 = getenv(e0);
	if (s0 == 0) {
	    fprintf(stderr, ("Environment variable %s needs to be set;"
			     " Seems not Open-MPI.\n"), e0);
	    fflush(0);
	    abort();
	}

	int v0;
	int cc0 = sscanf(s0, "%d%c", &v0, gomi);
	if (cc0 != 1) {
	    fprintf(stderr, "Bad environment variable %s (%s).\n", e0, s0);
	    fflush(0);
	    abort();
	}
	if (v0 < 0) {
	    fprintf(stderr, "Bad environment variable %s (%s).\n", e0, s0);
	    fflush(0);
	    abort();
	}

	char *e1 = "OMPI_MCA_orte_ess_num_procs";
	char *s1 = getenv(e1);
	if (s1 == 0) {
	    fprintf(stderr, ("Environment variable %s needs to be set;"
			     " Seems not Open-MPI.\n"), e1);
	    fflush(0);
	    abort();
	}

	int v1;
	int cc1 = sscanf(s1, "%d%c", &v1, gomi);
	if (cc1 != 1) {
	    fprintf(stderr, "Bad environment variable %s (%s).\n", e1, s1);
	    fflush(0);
	    abort();
	}
	if (v1 < 0) {
	    fprintf(stderr, "Bad environment variable %s (%s).\n", e1, s1);
	    fflush(0);
	    abort();
	}

	if (v0 >= v1) {
	    fprintf(stderr, ("Environment variables %s=%d and %s=%d"
			     " have bad values.\n"), e0, v0, e1, v1);
	    fflush(0);
	    abort();
	}

	vpid = v0;
	nprocs = v1;
    }

    /* Start processing. */

    int verbosity;
    int xargc;
    char **xargv;
    if (argc >= 3 && strncasecmp(argv[1], "-v", 2) == 0) {
	if (argv[1][2] == 0) {
	    verbosity = 2;
	} else if (argv[1][2] == '0') {
	    verbosity = 0;
	} else if (argv[1][2] == '1') {
	    verbosity = 1;
	} else if (argv[1][2] == '2') {
	    verbosity = 2;
	} else if (argv[1][2] == '3') {
	    verbosity = 3;
	} else {
	    verbosity = 2;
	}
	xargc = (argc - 2);
	xargv = &argv[2];
    } else {
	verbosity = -1;
	xargc = (argc - 1);
	xargv = &argv[1];
    }

    int master = (nprocs - 1);
    if (vpid == master) {
	/* MASTER */

	if (xargc < 1) {
	    fprintf(stderr, "USAGE: %s python-script-file\n", argv[0]);
	    fflush(0);

	    int nprocs, rank, thlv;
	    MPI_Init_thread(&argc, &argv, MPI_THREAD_SERIALIZED, &thlv);
	    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);
	    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
	    MPI_Abort(MPI_COMM_WORLD, 1);
	    exit(1);
	}

	char *cmd = xargv[0];
	FILE *f = fopen(cmd, "r");
	if (f == 0) {
	    char ee[80];
	    snprintf(ee, sizeof(ee), "fopen(%s): %s\n",
		     cmd, strerror(errno));
	    fprintf(stderr, ee);
	    fprintf(stderr, "USAGE: %s python-script-file\n", argv[0]);
	    fflush(0);

	    int nprocs, rank, thlv;
	    MPI_Init_thread(&argc, &argv, MPI_THREAD_SERIALIZED, &thlv);
	    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);
	    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
	    MPI_Abort(MPI_COMM_WORLD, 1);
	    exit(1);
	}

	Py_SetProgramName(argv[0]);
	Py_Initialize();
	PyRun_SimpleFile(f, cmd);
	Py_Finalize();
	return 0;
    } else {
	/* WORKERS */

	/* The actions of workers should exactly match with the master
	   in Python.  It assumes MPI4PY itself only does local MPI
	   operations. */

	int cc;

	int nprocs, rank, thlv;
	MPI_Init_thread(&argc, &argv, MPI_THREAD_SERIALIZED, &thlv);
	MPI_Comm_size(MPI_COMM_WORLD, &nprocs);
	MPI_Comm_rank(MPI_COMM_WORLD, &rank);

	kmr_init();

#pragma omp parallel
	{
	    sleep(0);
	}

	KMR *mr = kmr_create_context(MPI_COMM_WORLD, MPI_INFO_NULL, 0);
	assert(mr != 0);

	mr->trace_map_spawn = (verbosity > 0);

	MPI_Comm splitcomms[4];
	cc = kmr_split_swf_lanes(mr, splitcomms, master, 0, 1);
	assert(cc == MPI_SUCCESS);

	cc = kmr_init_swf(mr, splitcomms, master);
	assert(cc == MPI_SUCCESS);

	if (verbosity != -1) {
	    kmr_set_swf_verbosity(mr, verbosity);
	}
	cc = kmr_detach_swf_workers(mr);
	assert(cc == MPI_SUCCESS);

	/* NEVER COMES HERE. */

	return 0;
    }
}
